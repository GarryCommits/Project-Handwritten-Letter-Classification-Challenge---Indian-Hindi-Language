# -*- coding: utf-8 -*-
"""Handwritten Letter Classification Challenge - Indian Hindi Language.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eb2SbUyReyBGUZzowREWVXNeIgxLqa3z
"""

import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from PIL import Image
import matplotlib.pyplot as plt

# Path to the dataset
DATASET_DIR = "/content/drive/MyDrive/PGP Datasets/Indian Letter Handwritten Recognition/DatasetIND"

IMG_SIZE = 28
images = []
labels = []

for label in os.listdir(DATASET_DIR):
    class_dir = os.path.join(DATASET_DIR, label)
    print(f"üìÅ Checking folder: {class_dir}")
    if os.path.isdir(class_dir):
        for file in os.listdir(class_dir):
            img_path = os.path.join(class_dir, file)
            try:
                img = Image.open(img_path).convert('L')  # 'L' mode = grayscale
                img = img.resize((IMG_SIZE, IMG_SIZE))
                img_array = np.array(img)
                images.append(img_array)
                labels.append(label)
            except Exception as e:
                print(f"‚ùå Could not load image: {img_path} | Error: {e}")

# Convert to NumPy and normalize
X = np.array(images).astype("float32") / 255.0
X = X.reshape(-1, IMG_SIZE, IMG_SIZE, 1)
y = np.array(labels)

print("‚úÖ Loaded images:", X.shape)
print("‚úÖ Labels:", y.shape)

# Encode class labels
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)

print("‚úÖ Train set:", X_train.shape, y_train.shape)
print("‚úÖ Test set:", X_test.shape, y_test.shape)

from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import seaborn as sns

# Flatten the image data for PCA/t-SNE
X_flat = X.reshape(X.shape[0], -1)  # from (1339, 28, 28, 1) to (1339, 784)

# Reduce dimensions with PCA first
pca = PCA(n_components=50)
X_pca = pca.fit_transform(X_flat)

print("PCA shape:", X_pca.shape)

# Now apply t-SNE to PCA-reduced data
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
X_tsne = tsne.fit_transform(X_pca)

print("t-SNE shape:", X_tsne.shape)

# Plot the t-SNE output
plt.figure(figsize=(10, 7))
sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=labels, palette="tab10", legend='full')
plt.title("t-SNE Visualization of Indian Handwritten Letters")
plt.xlabel("t-SNE 1")
plt.ylabel("t-SNE 2")
plt.show()

# Model 1: Train an SVM Classifier (on flattened image vectors)
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

# Flatten train/test data
X_train_flat = X_train.reshape(X_train.shape[0], -1)
X_test_flat = X_test.reshape(X_test.shape[0], -1)

# Train SVM
svm = SVC(kernel='rbf', C=10, gamma=0.01)
svm.fit(X_train_flat, y_train)

# Predict & Evaluate
y_pred_svm = svm.predict(X_test_flat)
print("üéØ SVM Accuracy:", accuracy_score(y_test, y_pred_svm))
print(classification_report(y_test, y_pred_svm, target_names=encoder.classes_))

# Model 2: Train a Neural Network (CNN)
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical

# One-hot encode labels for neural net
y_train_cat = to_categorical(y_train)
y_test_cat = to_categorical(y_test)

# Build CNN
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(2, 2),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(10, activation='softmax')  # 10 classes
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train_cat, epochs=10, batch_size=32, validation_split=0.2)

# Evaluate Neural Network
loss, accuracy = model.evaluate(X_test, y_test_cat)
print("üéØ CNN Test Accuracy:", accuracy)

# Plot CNN Accuracy & loss
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title("CNN Accuracy")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title("CNN Loss")
plt.legend()

plt.show()

# Hyperparameter Tuning (Model Tuning)
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [1, 10, 100],
    'gamma': [0.001, 0.01, 0.1],
    'kernel': ['rbf']
}

grid = GridSearchCV(SVC(), param_grid, cv=3, verbose=2, n_jobs=-1)
grid.fit(X_train_flat, y_train)

print("‚úÖ Best SVM Parameters:", grid.best_params_)
print("üéØ Best CV Accuracy:", grid.best_score_)

# Save the Best CNN Model (Optional)
model.save("best_cnn_model.h5")

# Final Evaluation Report
# Create a simple evaluation summary:
print("üîç SVM Accuracy:", accuracy_score(y_test, y_pred_svm))
print("üîç CNN Accuracy:", accuracy)